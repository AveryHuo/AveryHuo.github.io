<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>GAMES104-NOTE4 | Avery的城堡</title><meta name="keywords" content="Engine"><meta name="author" content="霍家鹏"><meta name="copyright" content="霍家鹏"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1. 引擎中的GamePlay玩法 GamePlay is Everything 1.1 总览 • Event Mechanism • Script System • Visual Script • Character, Control and Camera GamePlay的挑战  各个子系统之间的协作 对于一些游戏中，可能存在各种类型玩法 如何针对市场情况，做出快速迭代  1.2 Event">
<meta property="og:type" content="article">
<meta property="og:title" content="GAMES104-NOTE4">
<meta property="og:url" content="http://www.hjp.wiki/2023/02/01/GAMES104_NOTE4/index.html">
<meta property="og:site_name" content="Avery的城堡">
<meta property="og:description" content="1. 引擎中的GamePlay玩法 GamePlay is Everything 1.1 总览 • Event Mechanism • Script System • Visual Script • Character, Control and Camera GamePlay的挑战  各个子系统之间的协作 对于一些游戏中，可能存在各种类型玩法 如何针对市场情况，做出快速迭代  1.2 Event">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://www.hjp.wiki/img/image-20230106174810993.png">
<meta property="article:published_time" content="2023-02-01T14:43:40.000Z">
<meta property="article:modified_time" content="2023-02-18T09:36:27.000Z">
<meta property="article:author" content="霍家鹏">
<meta property="article:tag" content="Engine">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.hjp.wiki/img/image-20230106174810993.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://www.hjp.wiki/2023/02/01/GAMES104_NOTE4/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 霍家鹏","link":"链接: ","source":"来源: Avery的城堡","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'GAMES104-NOTE4',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-18 17:36:27'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/my.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">99</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Avery的城堡</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">GAMES104-NOTE4</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-02-01T14:43:40.000Z" title="发表于 2023-02-01 22:43:40">2023-02-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-18T09:36:27.000Z" title="更新于 2023-02-18 17:36:27">2023-02-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%95%E6%93%8E/">引擎</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="GAMES104-NOTE4"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><h1>1. 引擎中的GamePlay玩法</h1>
<p>GamePlay is Everything</p>
<h2 id="1-1-总览">1.1 总览</h2>
<p>• Event Mechanism<br>
• Script System<br>
• Visual Script<br>
• Character, Control and Camera</p>
<p>GamePlay的挑战</p>
<ol>
<li>各个子系统之间的协作</li>
<li>对于一些游戏中，可能存在各种类型玩法</li>
<li>如何针对市场情况，做出快速迭代</li>
</ol>
<h2 id="1-2-Event-Mechanism">1.2 Event Mechanism</h2>
<ol>
<li>Publish-subscribe Pattern</li>
</ol>
<ul>
<li>发送者 -&gt; 事件注册到Dispatcher</li>
<li>Dispatcher送到各个GO中</li>
<li>GO返回Callback<br>
因此需要三个组件：<br>
组件1： Event Definition</li>
<li>这里的问题是，游戏的玩法多样性导致无法从程序层面预先定义好类型。UE的解决方案下是允许自定义类，生成可编辑的界面。</li>
<li>但另一个问题是，这样还是会需要重编译代码，对于UE来说，允许一种C++代码编译出的DLL的注入机制。</li>
</ul>
<p>组件2：Callback Registration<br>
预先注册一个callback函数句柄，由某个时刻被Invoke。</p>
<ul>
<li>问题，生命周期的问题，callback安全性问题，在invoke原指针地址已经被回收，wild point</li>
<li>强引用的做法：callback注册在对象不可能销毁！ 这样内存可能会越来越大。。</li>
<li>弱引用的做法：对象可以被销毁，但Invoke时，进行判定！</li>
</ul>
<p>组件3：Event Dispatching</p>
<ul>
<li>立即模式，父类的函数会一直等到callback后才能继续执行<br>
问题，多同步的情况难以处理，对于游戏环境的添加特效，如果加载的事件放成立即，则会花费大量的事件在发送事件</li>
<li>消息队列， 先放到队列中，在未来某个时间点按顺序（或优先级）执行</li>
</ul>
<ol>
<li>
<p>需要解决Event序列化和反序列化的问题，利用反射的机制，将Event与内存块之间进行相互转换。</p>
</li>
<li>
<p>进一步的优化Event的内存，用Ring Buffer的方法，通过移动指针来管理内存池。<br>
<img src="/img/image-20230106174810993.png" alt="image-20230106174810993"></p>
</li>
<li>
<p>消息队列进行分类， 对于复杂的Event来说，做出多个Dispatcher，多个Queue。如Net Event Queue, Combat Event Queue等，加强处理效率，也更易维护。</p>
</li>
<li>
<p>问题1， 队列无法保存顺序问题</p>
</li>
<li>
<p>问题2， 时效性问题，需要至少等一帧，无法立即处理。 如对于ACT游戏来说，实时性要求较高的情况。</p>
</li>
</ol>
<blockquote>
<p>Event不建议添加优先级机制，这会导致耦合度的增加，同时也不符合dispatcher的优势，更难以处理并行化的设计。设置了优先级就会包含了大量的假设，也加大了与dispatcher的耦合度。</p>
</blockquote>
<h2 id="1-3-Game-Logic的语言">1.3 Game Logic的语言</h2>
<ul>
<li>
<p>编译型语言构建游戏逻辑<br>
问题1： 更新迭代困难，每次修改都要重编<br>
问题2： 使用门槛高，设计（非开发）人员几乎无法参加</p>
</li>
<li>
<p>脚本语言，解释型语言<br>
在一个虚拟机的环境中运行<br>
<img src="/img/image-20230109104104021.png" alt="image-20230109104104021"></p>
</li>
<li>
<p>脚本管理系统<br>
方案一： Native语言控制游戏主世界，拥有更高的性能。<br>
方案二： 使用脚本语言控制<br>
实际情况下，在重度脚本依赖的游戏中，GO的创建管理多数在于脚本中，因为游戏逻辑中GO的使用控制是相对更多的，而脚本主控的也是游戏逻辑部分，编译型语言则是提供基础功能的接口。</p>
</li>
</ul>
<blockquote>
<p>脚本语言的缺点是慢，但借助JIT（just-in-time），一边运行一边编译，以此种方式优化整个性能，甚至可以超过编译语言。<br>
魔兽世界- 使用LUA</p>
</blockquote>
<h2 id="1-4-Visual-Scripting">1.4 Visual Scripting</h2>
<p>可视化编程语言</p>
<blockquote>
<p>引擎的本质是生产力工具</p>
</blockquote>
<ul>
<li>将整个脚本语言转为可视化的过程</li>
<li>难点：主要还是团队协作的问题，这里有规范性的问题，debugger机制，手动整理过程低效耗时</li>
<li>本质上，就是可视化的脚本，需要能与脚本之间互相转换，但与脚本并不是一个替代关系。而更好的是一个互补的情况</li>
</ul>
<h2 id="1-5-3C系统">1.5 3C系统</h2>
<p>Character, Control &amp; Camera<br>
形成了游戏的体验的核心</p>
<ul>
<li>Character: 首先就是Movement，对于3A游戏来说，移动会有很多的细节。 其次是与环境的互动。一般来说由 状态机来实现。</li>
<li>Control: 针对不同的设备的Input做出不同的对应响应，同时也需要反馈机制，另外对于一些ACT游戏，还需要设计不同的按键组合，这里有Chords，同时按下某些键，触发一个独特的行为，有Key Sequences，保留一段玩家的操作记录，以触发一个游戏行为。</li>
</ul>
<blockquote>
<p>多数的FPS游戏都配有一个吸咐系统，如果不做吸咐的话，由于操作的延迟可能会有100MS+，导致难以操作。</p>
</blockquote>
<ul>
<li>Camera: 当角色在跑动走动时，相机也会随着发生变化。非常重要的一方面，根据生物学的原理，让相机表达出更好的主观感受。</li>
</ul>
<h1>2. AI</h1>
<h2 id="2-1-Outline">2.1 Outline</h2>
<p>AI Basic<br>
• Navigation<br>
• Steering<br>
• Crowd Simulation<br>
• Sensing<br>
• Classic Decision Making Algorithms<br>
Advanced AI<br>
• Planning and Goals<br>
• Machine Learning</p>
<h2 id="2-2-Navigation">2.2 Navigation</h2>
<p>导航系统分为 Map representation -&gt; Path finding -&gt; Path smoothing</p>
<h3 id="2-2-1-Map-representation-Walkable-Area">2.2.1 Map representation - Walkable Area</h3>
<p>确定所有可以去的区域<br>
表达Walkable Area的格式：</p>
<ol>
<li>Waypoint Network 路点网络</li>
<li>Grid</li>
<li>Navigation Mesh</li>
<li>Sparse Voxel Octree 空间八叉树<br>
有些情况需要在游戏中使用多种格式</li>
</ol>
<ul>
<li>Waypoint Network</li>
</ul>
<ol>
<li>找最近的点。Find the nearest points to get on and off the network</li>
<li>Plan the path on the waypoint network<br>
实现容易，但限制性较多，路点选择需要手动介入</li>
</ol>
<ul>
<li>
<p>Grid<br>
统一标准划分，统一的格子形状。<br>
实现容易，更易于更新，统一的数据格式。缺点是存储空间造成浪费，在Grid中移动时，可能会造成较严重的cache miss，另外实现桥路和地下叠加路线较为复杂，像素的精准度问题。</p>
</li>
<li>
<p>Navigation Mesh<br>
解决重叠路面问题，对比waypoint，这里使用的是面覆盖的方式，且支持3D walkable surface。更加精准，更快的寻路，动态性更好。但生成较为复杂，对于3维空间来说无法处理<br>
使用Polygon，且必须得是凸Polygon(Convex Polygon)，而不是凹Polygon(Concave Polygon)。<br>
原因：</p>
</li>
</ul>
<ol>
<li>Pathfinding generates a series of polygon<br>
(Polygon Corridor) need to walk through，寻路形成一系列的多边形走廊，可能出现在多边形之外。</li>
<li>Convexity guarantees the final path is limited in<br>
the polygon and two adjacent polygons have only<br>
one common edge (Portal)。 两两个多边形之间仅有一个共享的边</li>
</ol>
<ul>
<li>Sparse Voxel Octree<br>
对于空战游戏，就可以使用八叉树的分类方式处理3维空间。问题是存储的空间要求较高，寻路较为困难<br>
<img src="/img/image-20230110113200418.png" alt="image-20230110113200418"></li>
</ul>
<h3 id="2-2-2-Pathfinding">2.2.2 Pathfinding</h3>
<p>所谓寻路的目标，1是找到一条可通达的道路，2是尽可能的找到相对近的路。</p>
<ul>
<li>
<p>广度优先算法<br>
较费时</p>
</li>
<li>
<p>深度优先算法<br>
较费时</p>
</li>
<li>
<p>Dijkstra<br>
每次遍历所有相邻的未访问过的点，找出与旧的点距离最短的点，直到找终点<br>
<img src="/img/image-20230110114014623.png" alt="image-20230110114014623"></p>
</li>
<li>
<p>A星<br>
基本基于Dijkstra的思路，额外加入一个新的启发函数，cost等于source+启发<br>
这里的启发函数可以理解为方向，如两点之间的直线距离，任意路径点与此直线距离的得到最终的cost.<br>
<img src="/img/image-20230110114632885.png" alt="image-20230110114632885"></p>
</li>
</ul>
<p>Grid的启发算法：</p>
<p><img src="/img/image-20230110115227819.png" alt="image-20230110115227819"></p>
<p>NavMesh的A星启发算法：</p>
<ol>
<li>
<p>一般使用边线的中点<br>
<img src="/img/image-20230110115108325.png" alt="image-20230110115108325"></p>
</li>
<li>
<p>计算欧拉距离<br>
<img src="/img/image-20230110115404018.png" alt="image-20230110115404018"></p>
</li>
</ol>
<p>Heuristic的行为：<br>
h(n) 过低，更慢，但容易找到最短路径，过高，更快点达到终点</p>
<h3 id="2-2-3-Path-smoothing">2.2.3 Path smoothing</h3>
<p>让AI走得更真实更自然</p>
<ul>
<li>Funnel Algorithm<br>
类似于人走路的算法实现，先从一个点看向通道，找出相邻边的多边形，再判断是否与能完全包裹多边形，是则往下个多边形减少范围。 否则找到与下个多边形最近的边来减少范围查找。<br>
<img src="/img/image-20230110163319224.png" alt="image-20230110163319224"></li>
</ul>
<p><img src="/img/image-20230110163352132.png" alt="image-20230110163352132"></p>
<h3 id="2-2-4-NavMesh的生成">2.2.4 NavMesh的生成</h3>
<ul>
<li>Voxelization  体素化自动生成NavMesh</li>
</ul>
<blockquote>
<p>库Raycast: 先让世界进行体素化生成各个Voxel，标记出能通行的区域，通过相邻的Voxel不能相差的方式找出。 再从所有的walkable voxel中找出所有的Edge，再按这些的Edge生成Distance Field 图，每个voxel找最近的edge，离edge的点最远的点找出，再从这个点向外扩散，以此形成 Distance Field，即距离场。更细节的处理，比如重叠的区域需要做剔除。</p>
</blockquote>
<ul>
<li>
<p>Region Segmentation</p>
<p><img src="/img/image-20230118114737725.png" alt="image-20230118114737725"></p>
<p><img src="/img/image-20230118114746362.png" alt="image-20230118114746362"></p>
<p><img src="/img/image-20230118114755560.png" alt="image-20230118114755560"></p>
</li>
<li>
<p>Mesh生成<br>
通过 Region Segmentation生成 Mesh</p>
<p><img src="/img/image-20230118114847714.png" alt="image-20230118114847714"></p>
</li>
</ul>
<h3 id="2-2-5-NavMesh的特性">2.2.5 NavMesh的特性</h3>
<ul>
<li>
<p>为不同的地形标记NavMesh<br>
如水面，地面，沙面等，对不同的地形发送事件以做不同的游戏逻辑处理</p>
</li>
<li>
<p>Tile<br>
对于场景物体来说，运用NavMesh的特性，生成不同的Tile，而不是完全更新NavMesh</p>
</li>
<li>
<p>Off-mesh Link<br>
允许在不同的地形之间的特殊行为。 因为在基础的NavMesh不足的情况，需要手动建立不同地形间的连接线，实现地形间的穿梭</p>
</li>
</ul>
<h2 id="2-3-Steering">2.3 Steering</h2>
<p>转向系统<br>
分为三种行为：</p>
<ul>
<li>
<ol>
<li>Seek Flee 目标转向  追寻一个目标的运动<br>
追踪与巡逻。 输入：自己位置与目标位置  输出：加速度</li>
</ol>
</li>
<li>
<ol start="2">
<li>VelocityMatch 速度转向<br>
追上目标的速度，当目标是静止和匀速时，较为容易，可由加速度公式求出，但如果目标的速度是不定时，需要在第一帧进行动态计算<br>
输入： 自己速度，目标速度，匹配时间    输出：加速度</li>
</ol>
</li>
<li>
<ol start="3">
<li>Align  角度转向<br>
与2类似，只是这里处理的是角速度的匹配。<br>
输入：自己的角度，目标角度   输出：角加速度</li>
</ol>
</li>
</ul>
<h2 id="2-4-Crowd-Simulation">2.4 Crowd Simulation</h2>
<p>群体（行为）模拟</p>
<h3 id="2-4-1-模拟方法">2.4.1 模拟方法</h3>
<ul>
<li>
<ol>
<li>Microscopic Models - Rule-based Models  微观方法<br>
预先为群体动态的个体制定一系列的规则来运动</li>
</ol>
</li>
<li>
<ol start="2">
<li>Macroscopic Models 宏观方法<br>
宏观上设计全局的LINE，让群体是沿着LINE行走和运动，但不考虑个体之间和与环境之间的关联</li>
</ol>
</li>
<li>
<ol start="3">
<li>Mesoscopic Models<br>
混合模式<br>
群体分组，个体可以按1的Role，整体就有一个LINE的规则，如RTS游戏控制小兵。</li>
</ol>
</li>
</ul>
<h3 id="2-4-2-碰撞避免">2.4.2 碰撞避免</h3>
<p>Collision Avoidance</p>
<ul>
<li>
<p>Force-based Models<br>
影响人群行为的社会心理和物理力量的混合， 个人的实际运动取决于所期望的速度及其与环境有关相互作用， 且可以模拟逃离人群恐慌的动态特征<br>
优点：适合人群模拟    缺点：与物理模拟类似，需要控制好模拟的步伐，以防模拟出错的情况</p>
</li>
<li>
<p>Velocity-base Models<br>
基于速度障碍生成的碰撞检测<br>
当两个物体要相遇时，会在速度域形成一个障碍，以让各自进行速度调整来避让碰撞。<br>
根据周边物体的信息，在速度域上做出决定。<br>
算法分类：</p>
</li>
</ul>
<ol>
<li>
<p>Velocity Obstacle (VO)<br>
计算它自己的躲避速度，假设其他座席无响应<br>
适用于静态和无响应的障碍<br>
容易跑过头<br>
可能会引起两个互相避让物体之间的振荡</p>
</li>
<li>
<p>Reciprocal Velocity Obstacle (RVO)<br>
假设另一方正在使用相同的决策过程(相互合作)<br>
双方都只走一半避免的碰撞的路<br>
仅保证两个物体间的无振荡和碰撞避免的情况</p>
</li>
<li>
<p>Optimal Reciprocal Collision Avoidance (ORCA)<br>
在2的基础上解决了群体的无振荡和碰撞避免的情况</p>
</li>
</ol>
<h2 id="2-5-Sensing-or-Perception">2.5 Sensing or Perception</h2>
<p>感知系统</p>
<h3 id="2-5-1-分类">2.5.1 分类</h3>
<p>内部信息+ 外部信息（静态空间信息，动态空间信息，角色信息）</p>
<ul>
<li>内部信息<br>
自身的信息，位置，血量，武器状态等</li>
<li>静态空间信息<br>
放置tactical（战略）点，让AI知道某些点可以做为更优的选择</li>
<li>动态空间信息<br>
Influence Map + Sight Area（视角区域）<br>
动态获取当前的游戏的信息或游戏行为变动或事件发出，以影响AI在战场上的行为。<br>
Game Objects<br>
动态获取场景物体的行为来影响 AI。</li>
</ul>
<h3 id="2-5-2-Sensing-模拟">2.5.2 Sensing 模拟</h3>
<p>1.空间中的光，声音， 气味<br>
2.最大行驶距离<br>
3.以不同的模式在空间和时间上衰减<br>
视线被障碍物挡住<br>
嗅觉范围会随着时间的推移而缩小<br>
4.辐射场可以模拟传感信号<br>
可以简化为影响图<br>
该字段覆盖的代理可以感知信息</p>
<blockquote>
<p>Sensing需要在引擎中设置开放的级别，以此控制对于性能方向的考虑和控制</p>
</blockquote>
<h2 id="2-6-Classic-Decision-Making-Algorithms">2.6 Classic Decision Making Algorithms</h2>
<h3 id="2-6-1-分类">2.6.1 分类</h3>
<p>六大算法：<br>
前向算法<br>
• Finite State Machine<br>
• Behavior Tree</p>
<p>• Hierarchical Tasks Network<br>
• Goal Oriented Action Planning<br>
• Monte Carlo Tree Search<br>
• Deep Learning</p>
<h3 id="2-6-2-Finite-State-Machine">2.6.2 Finite State Machine</h3>
<p>状态机，两个状态通过一定的条件进行切换状态。<br>
Transition + State+ Conditions<br>
State过多时，整个网络的复杂度会比较复杂。<br>
可以使用Hierachy FSM做为优化，但反应速度较慢，交互也会有问题。</p>
<p>整体来说：<br>
维护复杂，更新修改State难，重用性较差。</p>
<h3 id="2-6-3-Behavior-Tree">2.6.3 Behavior Tree</h3>
<p>Decision Tree 决策树<br>
用行为树来模拟出人类的思想决策</p>
<ul>
<li>Execution Nodes<br>
叶子结点，也是行为结点，处理一个单位的行为。拥有Fail, Success, Running三种状态</li>
<li>Control Nodes<br>
条件结点， 通过返回结果去决定后面的流程的走向<br>
Sequence： 从左往右（从上往下）依次执行子树，直达有结点返回Fail或Running, 返回，或者当所有的结点都成功，则返回成功<br>
Select: 从左往右（从上往下）依次执行子树，直达有结点返回Success或Running, 立即终止并返回，或者当所有的结点都失败，则返回失败<br>
Parallel: 逻辑上同时开始执行所有的子树，只要有任意M个子树执行成功，则返回成功，有N-M+1个结点失败则失败。 其他则返回Running</li>
</ul>
<p><img src="/img/image-20230118153654707.png" alt="image-20230118153654707"></p>
<ul>
<li>如何去Tick?<br>
要想人思考一样，每次从root结点去Tick， 每层的树结点按固定的顺序去执行，每个结点都要有固定的返回。</li>
</ul>
<blockquote>
<p>从头Tick的可能会导致性能问题，这里的优化的方法可以是去以某些激活结点去执行，同时添加一些事件去让BT从头执行。但这只是一种变体且需要明确约定好规则和设计。<br>
同时Running的结点可能有多个。</p>
</blockquote>
<ul>
<li>
<p>优化- Decorator<br>
对于一个结点执行固定的几类动作， 如•循环执行 •执行一次 •计时器 •时间限制 •值修改器几种</p>
</li>
<li>
<p>Precondition<br>
把条件合并到结点内，以简化BT的结构</p>
</li>
<li>
<p>Blackboard<br>
BT的内存区，用K-V去存储数据，让行为树的结点去存取数据。</p>
</li>
<li>
<p>BT的缺点<br>
每个TICK都从头就会有性能问题。交互条件越多就越多的消耗。</p>
</li>
</ul>
<h1>3. 高级AI</h1>
<h2 id="3-1-HTN-层次任务网络">3.1 HTN 层次任务网络</h2>
<p>Hierarchical tasks<br>
人类的行为是有意图的，有一定的目标性的，有层次的完成最终的目标。当设计师在构建AI时，更多的希望AI完成某一件事。</p>
<h3 id="3-1-1-框架：">3.1.1 框架：</h3>
<p><img src="/img/image-20230216182919512.png" alt="image-20230216182919512"></p>
<ol>
<li>World state: 包含一系列的属性，给计划者AI的一个状态。</li>
<li>Sensors: 传感器，接收世界的变更来改变World state. Perception</li>
<li>HTN Domain: 资源里加载进来，用于描述HT之间的关系。</li>
<li>Planner: 从World State和HTN Domain中制定计划</li>
<li>Plan Runner: 运行Plan，在任务完成后更新world state</li>
</ol>
<h3 id="3-1-2-任务：">3.1.2 任务：</h3>
<ul>
<li>
<ol>
<li>Primitive task: 原子任务<br>
先决条件Preconditions：  是否可以执行 + world properties的值是否可以满足。<br>
执行Action： 任务的执行内容<br>
影响Effects: 任务的执行会对world state properties的影响。 如改变某个数值</li>
</ol>
</li>
<li>
<ol start="2">
<li>Compound task: 复合任务<br>
包含多个methods， 每个method有自己优先级和先决条件，类似于BT的Selector的设定，按优先级找到满足先决条件的method执行。<br>
这里的method又会包含多个子的原子任务或复合任务，以此实现更加复杂的嵌套关系。</li>
</ol>
</li>
</ul>
<blockquote>
<p>通过这种方式的构建AI，更符合人类的思维方式</p>
</blockquote>
<h3 id="3-1-3-HTN-Domain：">3.1.3 HTN Domain：</h3>
<p>以Root Task为基，设定好各个method：</p>
<p><img src="/img/image-20230217112610021.png" alt="image-20230217112610021"></p>
<h3 id="3-1-4-Planing：">3.1.4 Planing：</h3>
<p>第一步：</p>
<ul>
<li>从Root task开始进入整个HTN。</li>
<li>按先决条件和优先级找到method<br>
第二步：</li>
<li>执行原子任务，如果不满足条件就找下个任务，没有下个任务了则直接往上层返回失败。</li>
<li>对于执行复合任务，解开任务内容，按先决条件和优先级进行执行</li>
</ul>
<blockquote>
<p>这里需要注意的是，复合任务在执行过程中会对world state进行一个副本拷贝，在执行中的任务假设为成功完成且修改到这块值。</p>
</blockquote>
<p>不断的重复第二步以完成整个Plan的制定。</p>
<h3 id="3-1-5-Run-Plan：">3.1.5 Run Plan：</h3>
<p>执行结束： 有一个任务执行失败了。或者是所有的都执行成功了。<br>
成功则更新world state数据，否则返回失败，进入Replan的过程</p>
<h3 id="3-1-6-Replaning：">3.1.6 Replaning：</h3>
<p>三种情况触发replan</p>
<ul>
<li>当前闲置状态</li>
<li>当前plan结束或失败了</li>
<li>传感器响应触发world state变更</li>
</ul>
<h3 id="3-1-7-总结：">3.1.7 总结：</h3>
<p>优点：<br>
与BT很类似，同时更高级别的抽象<br>
可以制定有长期影响的计划<br>
比BT更高效率<br>
缺点：<br>
玩家的行为难以预测，导致任务容易失败<br>
一些world state和任务的结果，对于设计者来说构建会有挑战性</p>
<h2 id="3-2-Goal-oriented-Action-Planning-GOAP">3.2 Goal-oriented Action Planning(GOAP)</h2>
<p>更加动态，backward planning而非forward</p>
<h3 id="3-2-1-框架">3.2.1 框架</h3>
<ul>
<li>World State和Sensors: 与HTN类似</li>
<li>Goal set： 目标集</li>
<li>Action set: 行为集</li>
<li>Planning: 一系列的行为计划<br>
<img src="/img/image-20230217142637232.png" alt="image-20230217142637232"></li>
</ul>
<h3 id="3-2-2-Goal-set">3.2.2 Goal set</h3>
<p>目标集是由所期望的最终的达成一个状态组成。<br>
每一个目标也可以表示为一系列的状态。</p>
<ul>
<li>precondition: 先决条件，决定选哪一个goal</li>
<li>Priority: 决定从多个goal中应该选哪个</li>
</ul>
<h3 id="3-2-3-Action-set">3.2.3 Action set</h3>
<p>包含一系列的Action，这里的Action与HTN的原子任务类似。<br>
但多一个Cost概念，其中表示这一个Action所需的消耗值，由开发者设定。</p>
<h3 id="3-2-4-执行流程">3.2.4 执行流程</h3>
<ol>
<li>Goal Set中找到第一个执行的GOAL</li>
<li>从第一个GOAL里的precondition里的world state中，与当前world state比较，得出不满足的条件，放到Unsatisfied States列表中</li>
<li>从Unsatisfied State顶部取出一个state，从Action set找出可以得到此state的一个Action，如果找到了，则把这个state从unsatisfied State列表中取出，把Action加入Plan stack中。</li>
<li>从Plan stack中拿到此Action，检测他的precondition条件，如果不满足，同样执行2，3这两步。<br>
最终的目标是清空Stack of Unsatisfied States.</li>
</ol>
<h3 id="3-2-5-Build-States-Action-Cost-Graph">3.2.5 Build States-Action-Cost Graph</h3>
<p>把GOAP转化为图的问题。 构建为有向图。<br>
图的结点：连接的各个States， 是一组state的condition值<br>
图的边：Action<br>
图的距离：Cost<br>
核心的问题： 从Goal到当前状态的一条最短路径！<br>
使用A<em>算法解决，这里的启发函数可以使用unsatisfied states的个数。 但A</em>并不保证找到最优解。但这不影响，实际上AI也可以更灵活</p>
<h3 id="3-2-6-总结">3.2.6 总结</h3>
<p>优点：<br>
目标与行为分开了，较HTN更加灵活，同时HTN也容易犯precondition/effect不匹配的错误<br>
缺点：<br>
较BT/FSM/HTN来说，消耗更大了，计算更慢。<br>
也需要一个很好的制定world state和action effect</p>
<h2 id="3-3-Monte-Carlo-Tree-Search">3.3 Monte Carlo Tree Search</h2>
<p>蒙特卡洛树搜索<br>
2006年由Enrico Fermi 提出的方法，第一个要解决的目标就是围棋。</p>
<blockquote>
<p>把问题抽象成数学模型，因为计算机只能处理数学模型</p>
</blockquote>
<h3 id="3-3-1-模型">3.3.1 模型</h3>
<ul>
<li>State: 当前的状态，通过一个node表示</li>
<li>Action: AI的一步操作，表示为一个edge</li>
<li>State Transfer: 从一个State到另一个State的Action</li>
<li>State Space: A Tree Structured State Space。 一系列通过action与current state连通的states集合。根结点就是current state</li>
</ul>
<p>整个模型的输入就是current state，构建出State Space， 输出就是从这个State Space中找出从current state 到best state的</p>
<h3 id="3-3-2-模拟">3.3.2 模拟</h3>
<ul>
<li>
<p>模拟<br>
根据默认策略从状态节点运行以产生结果</p>
</li>
<li>
<p>以围棋为例<br>
从状态中随机移动直到游戏结束<br>
根据结果返回1(赢)或0(输)</p>
</li>
<li>
<p>默认策略<br>
有意义但快速的游戏规则或神经网络</p>
</li>
</ul>
<h3 id="3-3-3-对于state的评估">3.3.3 对于state的评估</h3>
<p>评价因素<br>
•Q:模拟结果的累积<br>
•N:模拟次数</p>
<p>评估的因素： Q/N</p>
<h3 id="3-3-4-反向传播-Backpropagate">3.3.4 反向传播 Backpropagate</h3>
<p>对于当前模拟失败时，不仅仅要改自己的Q/N值，自己的父结点也要依次往上调整。 这样做了足够多的模拟时，所有探索过的结点都会有一组数据。</p>
<h3 id="3-3-5-迭代步骤">3.3.5 迭代步骤</h3>
<p>选择: 选择最有希望的“可扩展”节点 （most urgent “expandable” node）<br>
展开: 通过选择操作展开树<br>
模拟: 从新节点进行模拟并产生一个结果<br>
反向传播: 从新节点反向传播模拟结果</p>
<h3 id="3-3-6-选择-Selection">3.3.6 选择 Selection</h3>
<ul>
<li>
<p>“expandable” node：<br>
未探索完的结点</p>
</li>
<li>
<p>Selection<br>
Exploitation: 开发型的<br>
寻找有前景的领域<br>
选择Q/N值较高的子节点</p>
</li>
<li>
<p>Exploration: 探索型的，选择N值较小<br>
查看尚未充分采样的区域<br>
选择访问次数较少的孩子</p>
</li>
<li>
<p>如何平衡勘探与开发?<br>
使用UCB (Upper Confidence Bounds)公式<br>
𝑈𝐶𝐵𝑗:节点j的UCB值<br>
𝑄𝑗:通过节点j的所有播放的总奖励<br>
𝑁𝑗:节点j的访问次数<br>
𝑁:节点j的父节点被访问的次数<br>
𝐶:一个常数，调整以降低或增加探索执行的数量<br>
<img src="/img/image-20230217173155497.png" alt="image-20230217173155497"></p>
</li>
<li>
<p>如何选择最紧急的可扩展节点?<br>
始终从根节点开始搜索<br>
查找当前节点UCB值最高的子节点(有希望的子节点)<br>
设置有希望的子节点为当前节点<br>
重复以上步骤，直到当前节点是可展开的。设置当前节点为所选节点</p>
</li>
<li>
<p>扩展<br>
根据可用的操作，将一个或多个新的子节点添加到选定的节点<br>
子节点的值未知</p>
</li>
<li>
<p>基于扩展，模拟和反向传播</p>
</li>
<li>
<p>结束条件</p>
</li>
</ul>
<ol>
<li>设定内存大小以结束</li>
<li>设置计算时间</li>
</ol>
<ul>
<li>如何选择最好的招式?<br>
当前状态节点的“最佳”子节点<br>
Max child: 选择Q值最高的root child<br>
Robust child: :选择N值最多的根节点<br>
Max-Robust子: 选择访问次数和奖励最高的根子。如果不存在，则继续搜索，直到达到可接受的访问数<br>
安全子: 选择LCB（Lower confidence bound）最大化的子<br>
<img src="/img/image-20230217173320726.png" alt="image-20230217173320726"></li>
</ul>
<h3 id="3-3-7-总结">3.3.7 总结</h3>
<p>优点：<br>
MCTS代理行为多样<br>
Agent完全由自己做出决策<br>
可解决搜索空间大的问题<br>
更加适合于回合制，结果比较明确的游戏结果</p>
<p>缺点：<br>
对于复杂的游戏情况，如大多数即时游戏都很难设计动作和状态<br>
很难为大多数实时游戏建模</p>
<h2 id="3-4-Machine-Learning-Base">3.4 Machine Learning Base</h2>
<h3 id="3-4-1-分类：">3.4.1 分类：</h3>
<p>Supervised Learning: 监督式的学习<br>
Unsupervised Learning: 非监督式的学习<br>
Semi-supervised Learning: 半监督式的学习<br>
Reinforcement learning: 强化学习</p>
<h3 id="3-4-2-Supervised-Learning">3.4.2 Supervised Learning</h3>
<p>给定一些样本，从样本式学习，让机器可以进行识别。</p>
<h3 id="3-4-3-Unsupervised-Learning">3.4.3 Unsupervised Learning</h3>
<p>从无标签的数据中进行学习，进行分类操作。</p>
<h3 id="3-4-4-Semi-supervised-Learning">3.4.4 Semi-supervised Learning</h3>
<p>半监督式的，给定一系列的无标签数据，并加以做好标签的样本进行学习。</p>
<h3 id="3-4-5-Reinforcement-learning">3.4.5 Reinforcement learning</h3>
<p>强化学习，从与环境的交互过程中学习<br>
强化学习(RL)是机器学习的一个领域，研究智能代理在环境中应该如何采取行动，以最大化累积奖励的概念</p>
<ul>
<li>试错法搜索 Trial-and-error<br>
学习者必须通过尝试来发现哪些行为能产生最大的回报</li>
<li>延迟奖励<br>
行为可能会影响即时奖励、下一个场合和所有后续奖励</li>
</ul>
<h3 id="3-4-6-Markov-Decision-Process">3.4.6 Markov Decision Process</h3>
<p>马尔可夫决策过程</p>
<ul>
<li>基本元素：</li>
</ul>
<ol>
<li>Agent 代理<br>
学习者和决策者</li>
<li>Environment 环境<br>
与代理交互的对象，包括代理外部的所有内容</li>
<li>State<br>
Agent的观察，数据结构是人为设计的状态</li>
<li>Action<br>
行动是代理在游戏中所能表现的最小元素</li>
<li>Reward<br>
从环境传递到代理的每个时间步骤中接收的特殊信号</li>
</ol>
<ul>
<li>Mathematical Model 数学模型<br>
Probability of transition: 概率函数， 在采取行动a后，从s到s‘的转换概率<br>
Policy： AI系统的核心，类似于transition，policy也是一个随机变量。 AI的核心优化也是Policy， 无论是用神经网络还是<br>
Total reward： 从长远来看，它所获得的累积回报。 但是由于有一定的概率性，所以这里乘上一个系数。𝐺𝑡=𝑅𝑡+1+𝛾𝑅𝑡+2+𝛾2𝑅𝑡+3+⋯  这个系数平衡短期目标和长期目标，系数调大就是更偏向短期。</li>
</ul>
<h2 id="3-5-构建高级游戏AI">3.5 构建高级游戏AI</h2>
<p>在游戏AI中使用Machine Learning概率</p>
<blockquote>
<p>AlphaStar是DeepMind开发的一个电脑程序，可以玩电子游戏《星际争霸2》</p>
</blockquote>
<h3 id="3-5-1-DRL-example">3.5.1 DRL example</h3>
<p>Deep Reinforcement Learning 深度强化学习， 需要定义如下概念：</p>
<ol>
<li>State： 地图 + 游戏统计数据 + 各个单位实体 + 玩家数据</li>
<li>Action: 抽象化出来的行为， what+who+where+when</li>
<li>Reward： 奖励机制。 对于AlphaStar的方法直接获得的奖励+1/-1，配合以伪奖励输出与评论网络，范围控制在+1/-1以匹配直接奖励。 不同的奖励设定能帮助训练不同类型的agent，类似于“性格特征”。</li>
<li>NN design: neural network 神经网络，根据游戏的结构和逻辑，构建NN和网络选型</li>
<li>Trainning Strategy: 训练策略</li>
</ol>
<h3 id="3-5-2-NN">3.5.2 NN</h3>
<ul>
<li>
<p>整体结构- AlphaStar<br>
最下方的数据，往上放到不同的神经网络上，最终encode形成一个AI角色<br>
Decoder: 把结果翻译为人类可理解的行动<br>
<img src="/img/image-20230218170523513.png" alt="image-20230218170523513"></p>
</li>
<li>
<p>MLP Multi-Layer Perceptron 多层次感知器<br>
对于定长的数据，最简单的多层神经网络。给一个输入就有一个输出。</p>
</li>
<li>
<p>CNN Convolutional Neural Network 卷积神经网络<br>
如结构上的ResNet， 主要针对图像的识别，游戏中识别出哪里有角色（怪物或者敌方）。</p>
</li>
<li>
<p>Transfomer<br>
用于处理大量的时间上不定长的数据，持续处理变长的变量</p>
</li>
<li>
<p>LSTM Long-Short Term Memory<br>
像人类一样的思考，这里主要是使AI记录或删除旧的数据。 结构上，汇总上述三个网络处理完的数据。</p>
</li>
<li>
<p>NN Architecture Selection<br>
对于各类数据的选择，如定长数据使用MLP， 不定长使用LSTM，Transformer, 图像数据使用ResNet，Raycast或Mesh数据。</p>
</li>
</ul>
<h3 id="3-5-3-Trainning-Strategy">3.5.3 Trainning Strategy</h3>
<ul>
<li>使用Supervised Learning，先让AI模仿人类，当然首先得输入AI大量的数据。</li>
<li>Refeinforcement Learning</li>
<li>AlphaStar的方式：Self Play &amp; Adversarial</li>
</ul>
<ol>
<li>Main agents MA:<br>
目标: 成为最健壮和输出<br>
35%与自己打，50%与各过去的agents打， 15%跟过去的MA打</li>
<li>League exploiters LE:<br>
目标： 发现过去所有代理(MA, LE, ME)的弱点<br>
与所有过去的代理商(MA, LE, ME)的对抗</li>
<li>主要剥削者[ME]<br>
目标:发现当前MA代理的弱点<br>
针对当前的MA代理</li>
</ol>
<h3 id="3-5-4-RL与SL">3.5.4 RL与SL</h3>
<ul>
<li>
<p>SL： 监督式学习<br>
需要高质量的数据，有时也需要更明确的行为<br>
行为更像人类，但但可能不会超过人类专家数据， 往往也会有数据不足的问题</p>
</li>
<li>
<p>RL：强化学习<br>
强化学习通常被认为是最优的解决方案， 但RL model是困难的，而且算力的消耗较大，算出来的行为可能是不自然的。</p>
</li>
<li>
<p>如何选择： 如果奖励是足够密集的，用增强学习RL更易训练出好AI， 如果一个动作与奖励关联不直接，使用RL更难，SL则更适合。</p>
</li>
<li>
<p>Hybrid: 全局观<br>
机器学习是强大的。但的确比较昂贵。例如，deepmind花费2.5亿美元来完成alpha star，而复制需要1300万美元， 我们经常需要权衡DNN在类人点上的位置(整个战斗的一部分)。<br>
因此在适当的场合，可以组合各种策略来制作AI。</p>
</li>
</ul>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Engine/">Engine</a></div><div class="post_share"><div class="social-share" data-image="/img/image-20230106174810993.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/weixin.jpg" target="_blank"><img class="post-qr-code-img" src="/img/weixin.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/02/01/C++Review2/"><img class="prev-cover" src="/img/image-20230202174221474.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">C++ 复习2</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/06/GAMES104_NOTE3/"><img class="next-cover" src="/img/image-20221206143528541.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">GAMES104-NOTE3</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/10/19/GAMES104_NOTE1/" title="GAMES104-NOTE1"><img class="cover" src="/img/1608804894728513117.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-19</div><div class="title">GAMES104-NOTE1</div></div></a></div><div><a href="/2022/11/24/GAMES104_NOTE2/" title="GAMES104-NOTE2"><img class="cover" src="/img/image-20221123153956216.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-24</div><div class="title">GAMES104-NOTE2</div></div></a></div><div><a href="/2022/12/06/GAMES104_NOTE3/" title="GAMES104-NOTE3"><img class="cover" src="/img/image-20221206143528541.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-06</div><div class="title">GAMES104-NOTE3</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/my.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">霍家鹏</div><div class="author-info__description">Keep going! Keep study!</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">99</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/AveryHuo"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/AveryHuo" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:jackhamsir@sina.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎！欢迎来到我的博客空间，这里有我平时的学习心得和记录。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">1. 引擎中的GamePlay玩法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E6%80%BB%E8%A7%88"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 总览</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-Event-Mechanism"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 Event Mechanism</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-Game-Logic%E7%9A%84%E8%AF%AD%E8%A8%80"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 Game Logic的语言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-Visual-Scripting"><span class="toc-number">1.4.</span> <span class="toc-text">1.4 Visual Scripting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-5-3C%E7%B3%BB%E7%BB%9F"><span class="toc-number">1.5.</span> <span class="toc-text">1.5 3C系统</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">2. AI</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Outline"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Outline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Navigation"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 Navigation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-Map-representation-Walkable-Area"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.2.1 Map representation - Walkable Area</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-Pathfinding"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2.2 Pathfinding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3-Path-smoothing"><span class="toc-number">2.2.3.</span> <span class="toc-text">2.2.3 Path smoothing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-4-NavMesh%E7%9A%84%E7%94%9F%E6%88%90"><span class="toc-number">2.2.4.</span> <span class="toc-text">2.2.4 NavMesh的生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-5-NavMesh%E7%9A%84%E7%89%B9%E6%80%A7"><span class="toc-number">2.2.5.</span> <span class="toc-text">2.2.5 NavMesh的特性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Steering"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 Steering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-Crowd-Simulation"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 Crowd Simulation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-1-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95"><span class="toc-number">2.4.1.</span> <span class="toc-text">2.4.1 模拟方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-2-%E7%A2%B0%E6%92%9E%E9%81%BF%E5%85%8D"><span class="toc-number">2.4.2.</span> <span class="toc-text">2.4.2 碰撞避免</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-Sensing-or-Perception"><span class="toc-number">2.5.</span> <span class="toc-text">2.5 Sensing or Perception</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-1-%E5%88%86%E7%B1%BB"><span class="toc-number">2.5.1.</span> <span class="toc-text">2.5.1 分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-2-Sensing-%E6%A8%A1%E6%8B%9F"><span class="toc-number">2.5.2.</span> <span class="toc-text">2.5.2 Sensing 模拟</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-Classic-Decision-Making-Algorithms"><span class="toc-number">2.6.</span> <span class="toc-text">2.6 Classic Decision Making Algorithms</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-1-%E5%88%86%E7%B1%BB"><span class="toc-number">2.6.1.</span> <span class="toc-text">2.6.1 分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-2-Finite-State-Machine"><span class="toc-number">2.6.2.</span> <span class="toc-text">2.6.2 Finite State Machine</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-3-Behavior-Tree"><span class="toc-number">2.6.3.</span> <span class="toc-text">2.6.3 Behavior Tree</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">3. 高级AI</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-HTN-%E5%B1%82%E6%AC%A1%E4%BB%BB%E5%8A%A1%E7%BD%91%E7%BB%9C"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 HTN 层次任务网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-%E6%A1%86%E6%9E%B6%EF%BC%9A"><span class="toc-number">3.1.1.</span> <span class="toc-text">3.1.1 框架：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E4%BB%BB%E5%8A%A1%EF%BC%9A"><span class="toc-number">3.1.2.</span> <span class="toc-text">3.1.2 任务：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3-HTN-Domain%EF%BC%9A"><span class="toc-number">3.1.3.</span> <span class="toc-text">3.1.3 HTN Domain：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-4-Planing%EF%BC%9A"><span class="toc-number">3.1.4.</span> <span class="toc-text">3.1.4 Planing：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-5-Run-Plan%EF%BC%9A"><span class="toc-number">3.1.5.</span> <span class="toc-text">3.1.5 Run Plan：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-6-Replaning%EF%BC%9A"><span class="toc-number">3.1.6.</span> <span class="toc-text">3.1.6 Replaning：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-7-%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="toc-number">3.1.7.</span> <span class="toc-text">3.1.7 总结：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Goal-oriented-Action-Planning-GOAP"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 Goal-oriented Action Planning(GOAP)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-%E6%A1%86%E6%9E%B6"><span class="toc-number">3.2.1.</span> <span class="toc-text">3.2.1 框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-Goal-set"><span class="toc-number">3.2.2.</span> <span class="toc-text">3.2.2 Goal set</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-Action-set"><span class="toc-number">3.2.3.</span> <span class="toc-text">3.2.3 Action set</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-4-%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.4.</span> <span class="toc-text">3.2.4 执行流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-5-Build-States-Action-Cost-Graph"><span class="toc-number">3.2.5.</span> <span class="toc-text">3.2.5 Build States-Action-Cost Graph</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-6-%E6%80%BB%E7%BB%93"><span class="toc-number">3.2.6.</span> <span class="toc-text">3.2.6 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-Monte-Carlo-Tree-Search"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 Monte Carlo Tree Search</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.3.1.</span> <span class="toc-text">3.3.1 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-%E6%A8%A1%E6%8B%9F"><span class="toc-number">3.3.2.</span> <span class="toc-text">3.3.2 模拟</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3-%E5%AF%B9%E4%BA%8Estate%E7%9A%84%E8%AF%84%E4%BC%B0"><span class="toc-number">3.3.3.</span> <span class="toc-text">3.3.3 对于state的评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-4-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-Backpropagate"><span class="toc-number">3.3.4.</span> <span class="toc-text">3.3.4 反向传播 Backpropagate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-5-%E8%BF%AD%E4%BB%A3%E6%AD%A5%E9%AA%A4"><span class="toc-number">3.3.5.</span> <span class="toc-text">3.3.5 迭代步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-6-%E9%80%89%E6%8B%A9-Selection"><span class="toc-number">3.3.6.</span> <span class="toc-text">3.3.6 选择 Selection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-7-%E6%80%BB%E7%BB%93"><span class="toc-number">3.3.7.</span> <span class="toc-text">3.3.7 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-Machine-Learning-Base"><span class="toc-number">3.4.</span> <span class="toc-text">3.4 Machine Learning Base</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1-%E5%88%86%E7%B1%BB%EF%BC%9A"><span class="toc-number">3.4.1.</span> <span class="toc-text">3.4.1 分类：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2-Supervised-Learning"><span class="toc-number">3.4.2.</span> <span class="toc-text">3.4.2 Supervised Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-3-Unsupervised-Learning"><span class="toc-number">3.4.3.</span> <span class="toc-text">3.4.3 Unsupervised Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-4-Semi-supervised-Learning"><span class="toc-number">3.4.4.</span> <span class="toc-text">3.4.4 Semi-supervised Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-5-Reinforcement-learning"><span class="toc-number">3.4.5.</span> <span class="toc-text">3.4.5 Reinforcement learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-6-Markov-Decision-Process"><span class="toc-number">3.4.6.</span> <span class="toc-text">3.4.6 Markov Decision Process</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-%E6%9E%84%E5%BB%BA%E9%AB%98%E7%BA%A7%E6%B8%B8%E6%88%8FAI"><span class="toc-number">3.5.</span> <span class="toc-text">3.5 构建高级游戏AI</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-1-DRL-example"><span class="toc-number">3.5.1.</span> <span class="toc-text">3.5.1 DRL example</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-2-NN"><span class="toc-number">3.5.2.</span> <span class="toc-text">3.5.2 NN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-3-Trainning-Strategy"><span class="toc-number">3.5.3.</span> <span class="toc-text">3.5.3 Trainning Strategy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-4-RL%E4%B8%8ESL"><span class="toc-number">3.5.4.</span> <span class="toc-text">3.5.4 RL与SL</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/02/25/C++Review3/" title="C++ 复习3">C++ 复习3</a><time datetime="2023-03-06T10:28:34.000Z" title="更新于 2023-03-06 18:28:34">2023-03-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/02/20/C++11NewFeatures/" title="C++ 11新特性">C++ 11新特性</a><time datetime="2023-02-25T07:07:26.000Z" title="更新于 2023-02-25 15:07:26">2023-02-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/01/GAMES104_NOTE4/" title="GAMES104-NOTE4"><img src="/img/image-20230106174810993.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GAMES104-NOTE4"/></a><div class="content"><a class="title" href="/2023/02/01/GAMES104_NOTE4/" title="GAMES104-NOTE4">GAMES104-NOTE4</a><time datetime="2023-02-18T09:36:27.000Z" title="更新于 2023-02-18 17:36:27">2023-02-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/01/C++Review2/" title="C++ 复习2"><img src="/img/image-20230202174221474.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C++ 复习2"/></a><div class="content"><a class="title" href="/2023/02/01/C++Review2/" title="C++ 复习2">C++ 复习2</a><time datetime="2023-02-16T08:03:33.000Z" title="更新于 2023-02-16 16:03:33">2023-02-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/05/19/C++Review1/" title="C++ 复习1">C++ 复习1</a><time datetime="2023-02-01T07:38:50.000Z" title="更新于 2023-02-01 15:38:50">2023-02-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2023 By 霍家鹏</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a target="_blank" rel="noopener" href="http://beian.miit.gov.cn"><img class="icp-icon" src="icp图片"><span>粤ICP备2020091327号</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>